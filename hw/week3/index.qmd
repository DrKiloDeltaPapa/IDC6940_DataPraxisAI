---
title: "DataPraxisAI: Literature Review of *Predicting Student Dropout and Academic Success*"
subtitle: "Capstone(Spring 2026):week3_paper_review"
author: "Kareem D.Piper (Advisor: Dr.Shusen Pu)"
date: today
date-format: "MMMM D, YYYY"
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

#### Background/Motivation:

-   **Describe the context in which the research was conducted.**

    The research conducted by Realinho et al. (2022) is framed in the higher education setting, specifically, issues with students dropping out and the lack of academic success. According to Realinho et al. (2022) student attrition and academic failure does not only negatively affect the education institutions that students attend, but they create greater adverse societal issues. For example, when students drop out prior to completing a degree program it makes them less competitive concerning the job market, which then leads to economic dificiencies, which can lead to inadequate health care access and subsequent more dire socio-economic issues that often disproportionally affect marginalized demographics (Realinho et al., 2022). 

-   **What problem or gap in the existing literature does the paper aim to address?**

    Concerning the problem or gap in the existing literature on students dropping out of institutes of higher learning prior to completing their degrees Realinho et al. (2022) assert that there is no universal measure of "student dropout". Researchers who are produce work on higher education student attrition often have differing measures drop out, and most mesaure the phenomena at the macro level, which then creates issues concerning research comparison and understanding the nuances of the phenomena at a micro level (Realinho et al., 2022). To address the issue of disparate measures of student dropout and to better understand the features that contribute to it Realinho et al. (2022) proposed taking a micro perspective in that changes to student degree projection would be looked at regardless of time of occurence. Further, Realinho et al. (2022) compiled a unique data set that comprised of varied source data which they then used to train machine learning classifiers that predict student drop out and academic success.

-   **Discuss the significance of the research question and why it is important.**
      
    Realinho et al. (2022) asks, "What are the key factors that lead to student dropout?" Posed in a higher education setting this question is significant to all educational environments to include K-12 as many of factors correlate. Further the issue of predicting the root causes of stundent attrition and then using model results to directly drive changes to pedagogy or intervention is not typically explored (Realinho et al., 2022). 
    

#### Method Used:

-   **Summarize the methodologies employed by the authors.**

    As mentioned previously data for the study conducted by Realinho et al. (2022) stemmed for varied sources and were then compiled into one data set. According to Realinho et al. (2022), the sources of data were; 1. the institutions Academic Management System (internal data), 2.the sytem used for teaching and learning (internal data), 3. annual data from  the General Directorate of Higher Education (DGES) for admissions data through the National Competition for Access to Higher Education (CNAES) (external data), and 4. the Contemporary Portugal Database (PORDATA) used for macroeconomic data also (external).The data consited of undergraduate studnets enrolled in seventeen different academic programs for example,  journalism,  education, nursing, and management. According to Realinho et al. (2022), once all data sources were compiled into the final data set used in the study, it consited of (n = 4,424) cases and (n = 35) features. 
   
    The feautres fell into four main catagories; demographic, socioeconomic, macroeconomic, and academic. The target variable (i.e., dropout, enrolled or graduating), was multi-class. Concerning data preprocessing, Realinho et al. (2022) used a multi-tool approach. For example, initial data ingestion and preprocessing was done using tools such as Microsoft Access Databases, Microsoft's Visual Studio Basic, Microsoft Windows suit, and code processing and development in Python. According to Realinho et al. (2022), detailed data anlysis was done using tools from the Pandas library, Scikit-learn and Bokeh library for visualizations. The outputd of descritpive statitics were listed for all features in the form of tables that were parsed based on the major feature catagories. In conducting desctiptive statistics and assessing collinarity Realinho et al. (2022) did notice that several features, both within and between feature catagories showed high levels of multicolinarity, which they assed by means of a heat map and Pearson correlation coefficent using 0.7 as a cut point.The Pearson correlation coefficient $r$ measures the strength and direction of the linerar relationship between two features. A cut point $|r| \ge 0.07$ is considered a minimal threshold (see formula below).
$$
r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^{2}} \, \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^{2}}}
$$
    
    
    Where:
    * $r$ = The Pearson correlation coefficent, which measures the strength       and direction of the linear relationship between two features
    * $x_i$ = The observed value of the feature $X$ for the $i$-th               observation
    * $y_i$ = The observed value of the target $Y$ for the $i$-th                observation
    * $\bar{x}$ = The  mean of all values of the feature $X$
    * $\bar{y}$ = The mean of all values of the Target $Y$
    * $n$ = The total number of paired observations
    * $\sum_{i=1}^{n}$ = The summation over all observations from $i$ = 1        to $n$
    * $\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^{2}}$ = the standard deviation       for the feature $X$
    * $\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^{2}}$ = the standard deviation       for the target $Y$

    To assess wich features would be most important to the model, and to reduce the number of features Realinho et al. (2022) used the Permutation Feature Importance technique. Permutation Feature importance is calculated by oberving differences in differences in model error (i.e., decreases or increases) when feature values are permutated. If premutating feature values causes big differences  in the model error, the feature is important to the overal model (see formula below).
$$
PI_j = E_{\text{perm}(j)} - E_{\text{base}}
$$
    
    Where:
      * $PI_j$ = the permutation importnace score for the feaure $_j$
      * $E_{\text{base}}$ = the base model error (i.e., the performance            metric) computed on the orginal dataset
      * $E_{\text{perm}(j)}$ = the model error after random permutation of         the values of feature $_j$
      * $j$ = the index of the feature being evaluted
      * $E$ = the prediction error metric used for evaluation (e.g., mean          squared error, log loss or error rate)

    Concerning the target, Realinho et al. (2022) noted that there was significant class imbalnce, for example, the class label "Graduate" at (n = 2,209) represented 50% of the sample, while "Dropout" (n = 1,421) represented 32%, and "Enrolled" (n = 794) represented 18%. Thus to assess the error when conducting permutation Realinho et al. (2022), used F1-score as their metric, which they asserted is most adequate when dealing with an imbalnaced data set. The F1-score measures the accurcy of classification models assessing how well they predict the positive class by balancing both precision and recall. In the context of selecting "important" features, if permutation causes significant decreases in F1-score, then a feature is deemed important (see formula below).
$$
F_1 = \frac{2PR}{P + R}
$$
    
     Where:
      * $F_1$ = The $F1$-score which is the harmonic mean of precision             andreval
      * $P$ = Precision, which is the prroportion of predicted postives            that are true positives
      * $R$ = Recall, which is  the ratio of acutal postives that are              correctly predicted
      * $2$ = The weighting factor that yeilds the harmoic mean and it             penalizes extreme imbalances between $P$ and $R$

      According to Realinho et al. (2022) Permutation Feature Importance was applied to models suggested by the literature on multi-class prediction models, for example Random Forest(RF), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and Catagotical Boosting(CatBoost). RF is considred an ensemble learning method that builds upon many decision trees and then aggregates their predictiond thus improving model accuracy while reducing overfitting(20)(see RF formula below). XGBoost operates on the premise of gradient boosting and builds trees in a sequential manner where each new tree corrects the errors of the prevuious tree(21)(see XGBoost formula below). LightGBM also operates using a gradient boosting method but uses a histogram-based tree framework while it learns to scale large datasets (22)(see LightGBM formula below). Finally, CatBoost also uses gradient boosting method but is specfically designed to handle catagorical features (23)(see CatBoost formula below).
      
    **Random Forest Formula**
$$
\hat{y} = \frac{1}{B}\sum_{b=1}^{B} T_b(x)
$$

    Where:
      * $\hat{y}$ = The final predicted out of the RF model
      * $B$ = The total number of decision trees in the RF
      * $T_b(x)$ = The prediction of $b$-th decision tree for input $x$
      * $x$ = The feauture vector (i.e., the feature observation)
      * $\sum_{b=1}^{B}$ = The aggregation accross all trees
    
    
    **Extreme Gradient Boosting Formula**
$$
\hat{y}_i = \sum_{k=1}^{K} f_k(x_i), 
\quad f_k \in \mathcal{F}
$$

    Where:
      * 
      
      
    
      
    

      
      
      
-   **Explain how these methods are suited to address the research question.**

    The methods used by....

-   **Discuss any innovative approaches or techniques that are particularly noteworthy.**

    Concerning innovative proceesses...

#### Significance of the Work:

-   **Highlight the key findings and contributions of the paper.**

    The key findings of the paper...

-  **Explain why the results are important within the broader context of the field.**

    The resuts are important in regards to the broader context of the field.....

-   **Discuss the implications of the findings for future research or practice.**

    The findings put forth by .....

#### Connection to Other Work:

-   **Relate the paper to other relevant studies.**

    Conceptually....

-   **How does this paper build on or differ from previous work?**

    This paper builds on previous work concerning predicitng studnet attrition...

-   **Identify any references to seminal works or influential papers cited by the authors.**

    Concerning references to seminal and or influential works cited by ....

#### Relevance to Capstone Project:

-   **Discuss how the content of the paper might be relevant to your own capstone project.**

    The content of this paper directly realtes to my capstone....

-   **Identify any specific methods, theories, or findings that you might incorporate into your project.**

    Concerning the specific methods, theories and findings that I intentd to incorporate into my capstone project...

-   **Highlight any potential areas where your capstone could expand upon or diverge from the paper’s findings.**

    Concerning areas where my capstone could expand upon.... Concerning where my capstone diverges from....

#### 

##                               Reference

Aina, C., Baici, E., Casalone, G., & Pastore, F. (2022). The determinants of university dropout: A review of the *socio-economic literature. Socio-Economic Planning Sciences, 79, Article 101102.* https://doi.org/10.1016/j.seps.2021.101102

Andrade-Giron, D., et al.(2023). Predicting student dropout based on machine learning  and deep learning: A systematic review. *ICST Transaction on Scalable Information Systems, 10(5),1-11.*
https://doi.org/10.4108/eetsis.3586

Bargmann, C., Thiele, L., & Ksuggrld, S. (2022). Motivation matters: Predicting students’ career decidedness and intention to drop out after the first year in higher education. *Higher Education, 83(4), 845-861.* https://doi.org/10.1007/s10734-021-00707-6

Barramuno, M., Meza-Narvaez, C., & Galvez-Garcia, G. (2022). Prediction of student attrition risk using machine learning. *Journal of Applied Research in Higher Education, 14(3), 974-986.* https://doi.org/10.1108/JARHE-02-2021-0073

Delen, D., Davazdahemami, B., & Rasouli Dezfouli, E. (2024). Predicting and mitigating freshman student attrition: A local-explainable machine learning framework. Information Systems Frontiers, 26(2), 641-662. https://doi.org/10.1007/s10796-023-10397-3

Duan, D., Dai, C., & Tu, r. (2021). Research on the predicion of students' academic performance based on XGBoost. In *Proceedings of the 2021 Tenth International Conference on Educational Innovation through Technology (EITT)* (pp.316-319). IEEE https://doi.org/10.1109/EITT53287.2021.00068

Huo, H. et al. (2023). Predicting dropout for nontraditional undergraduate students: A machine learning approach. *Journal of College Student Retention:Research, Theory & Practice*, 24(4), 1054-1077.https://doi.org/10.1177/1521025120963821

Kok, C. L., Ho, C.K., Chen, L., Koh, Y. Y., & Tian, B. (2024). A novel predictive modeling for student attrition utilizing machine learning and sustainable big data analytics. *Applied Sciences, 14(21), Article 9633.* https://doi.org/10.3390/app14219633

Mduma, N. (2023). Data balancing techniques for predicting student dropout using machine learning. Data, 8(3), Article 49. https://doi.org/10.3390/data8030049

Mun, J., & Jo, M. (2023). Applying machine learning-based models to prevent university student dropouts. *Journal of Educational Evaluation*, 36(2), 289–313. https://doi.org/10.31158/JEEV.2023.36.2.289

Namoun, A., & Alshanqiti, A. (2020). Predicting student performance using data mining and learning analytics techniques: A systematic literature review. Applied Science, 11(1), Article 237. https://doi.org/10.3390/app11010237

Pek, R.Z., Ozyer, S.T., Elhage, T., Ozyer, T., & Alhajj, R. (2023). The role of machine learning in identifying student at-risk and minimizing failure. *IEEE Access, 11, 1224-1243.* https://doi.org/10.1109/ACCESS.2022.3232984

Parjwal, P., L. R., S., & V., K. (2024). Forcasting student attrition using machine learning. In *Proceedings of the 2024 4th Asian Conference on Innovation in Technology (ASIANCON)* (pp.1-7) IEEE.
https://doi.org/10.1109/ASIANCON62057.2024.10838214

Realinho, V., Machado, J., Baptista, L., & Martins, M. V. (2022). Predicting student dropout and academic success. Data, 7(11), Article 146. https://doi.org/10.3390/data7110146

Ridwan, A., Priyatno, A. M., & Ningsih, L. (2024). Predict Students' Dropout and Academic Success with XGBoost. *Journal of Education and Computer Applications*,*1*(2), 1-8.https://doi.org/10.69693/jeca.v1i2.13

Veliz Palomino, J.C., & Ortega, A. M. (2023). Dropout intentions in higher education: Systematic literature review. *Journal of Efficacy and Responsibility in Education and Science, 16(2), 149-158.*

https://doi.org/10.7160/eriesj.2023.160206

Yan, K. (2021). Student performance prediction using XGBoost method from macro perspective. In *Proceedings of the 2021 2nd International Conference on Computing and DataScience(CDS)*(pp.453-459).IEEE.https://doi.org/10.1109/CDS52072.2021.00084


