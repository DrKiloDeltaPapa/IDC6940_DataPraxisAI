---
title: "DataPraxisAI : Predicting Student Dropout and Academic Success: Methods + Data + Implementation"
subtitle: "Capstone(Spring 2026): Week4_and_Week5 Progress Report Assignment"
author: "Kareem D. Piper (Advisor: Dr.Shusen Pu)"
date: today
date-format: "MMMM D, YYYY"
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Project Goal and Research Questions:

-   **Project Goal**
    
    The goal of this capstone project is to test, assess and reprot the results various multi-class machine learning prediction models on their ability to accuratly predict studnet drop out in the higher edcation setting. According to Realinho et al. (2022) student attrition and academic failure does not only negatively affect the education institutions that students attend, but they create greater adverse societal issues. For example, when students drop out prior to completing a degree program it makes them less competitive concerning the job market, which then leads to economic dificiencies, which can lead to inadequate health care access and subsequent more dire socio-economic issues that often disproportionally affect marginalized demographics (Realinho et al., 2022). 
    
    However, accroding to Mduma (2023), the prediction of student dropout is a particularly challanging issues for education researchers, this is due in large part to class imblance. According to Mduma (2023), many real world datasets conering studnet dropout are largly skewed towards the reatined or enrolled classes. While many scholars pay particuliar attention to feature enigineering methods when using machine learning methodologies to address the issue of predicting student attrion Mduma (2023) asserted that class imbalance present limtations concering model accuracy and generalizabiltiy and thus framed their study aorund that issue. Thus, in this capstone project, I focus on the approproate data preprosing methods, feature egineering methods, and methods to adequalty address class imbalnce. Further, I specifically use machine learning models that have a proven track record in the litertue (e.g., Realinho et al., 2022; Ridwan et al., 2024) conerning their abiltiy to address multi-class datasets, for example Random Forest(RF), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and Catagotical Boosting (CatBoost).

    
## Research Questions
  
-   **Primary Research Question** 
    
    **RQ1:** Which multi-class machine learning classifier achieves the highest predictive performance as meaured by; accuracy, precesion, and F1-score when applied to the ([UCI Student Dropout Dataset](https://www.kaggle.com/datasets/willatran/original-dataset-for-pss4e6-from-uci-ml-repo/data))?
    
    To inform the primary research question of this study I asked the following sub-research questions:
    
    **RQ1a:** How do methods of addressing class imbalance affect the predictive performance of multi-class machine learning classfiers?
    
    **RQ1b:** How does the performance of gradient boosting models compare to tradtional ensable models in multi-class student attrition prediciton?
  
  
## Expanded Methods (updated from week3 paper review):

-   ***Methods***
  
    As the main research question of my study centers around comparing multi-class machine learning classifiers and addressing class imblance, to accuratly predict student dropout in a higher education setting, methods correlate directly to those used by (e.g., Mduma, 2023; Realinho et al., 2022). The methods outlined by Realinho et al. (2022), builds on previous work concerning predicitng student dropout by implementing a multi-class approach. While, the methods outlined by Mduma (2023) specifically use five varied data balancing technques paried with machine learning clssifiers to determine which pairing most accurately predcited student drop out. Thus the remainder of this methods section is broken down into to parts, 1. Multi-Class Methods and 2. Data Balancing Methods. In the final report for this capstone project, a detailed account of how these methods are incorparted into my project along with full machine learnining pipeline methods will be given.
    
-   **The Multi-Class Methods**
    
      The study conducted by Realinho et al. (2022) stemmed from varied sources and were then compiled into one data set. According to Realinho et al. (2022), the sources of data were; 1. the institutions Academic Management System (internal data), 2. the system used for teaching and learning (internal data), 3. annual data from  the General Directorate of Higher Education (DGES) for admissions data through the National Competition for Access to Higher Education (CNAES) (external data), and 4. the Contemporary Portugal Database (PORDATA) used for macroeconomic data also (external).The data consited of undergraduate studnets enrolled in seventeen different academic programs for example,  journalism,  education, nursing, and management. According to Realinho et al. (2022), once all data sources were compiled into the final data set used in the study, it consited of (n = 4,424) cases and (n = 35) features. 
   
    The feautres fell into four main catagories; demographic, socioeconomic, macroeconomic, and academic. The target variable (i.e., dropout, enrolled, or graduating), was multi-class. Concerning data preprocessing, Realinho et al. (2022) used a multi-tool approach. For example, initial data ingestion and preprocessing was done using tools such as Microsoft Access Databases, Microsoft's Visual Studio Basic, Microsoft Windows suit, and code processing and development was done in Python. According to Realinho et al. (2022), detailed data anlysis was done using tools from the Pandas library, Scikit-learn and Bokeh library for visualizations. The output of descritpive statitics were listed for all features in the form of tables that were parsed based on the major feature catagories. In conducting descriptive statistics and assessing collinarity Realinho et al. (2022) did notice that several features, both within and between feature catagories showed high levels of multicolinarity, which they assed by means of a heat map and Pearson correlation coefficent using 0.7 as a cut point.The Pearson correlation coefficient $r$ measures the strength and direction of the linerar relationship between two features. A cut point $|r| \ge 0.07$ is considered a minimal threshold (see formula below).
$$
r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^{2}} \, \sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^{2}}}
$$
    
    
    Where:
    * $r$ = The Pearson correlation coefficent, which measures the strength       and direction of the linear relationship between two features
    * $x_i$ = The observed value of the feature $X$ for the $i$-th               observation
    * $y_i$ = The observed value of the target $Y$ for the $i$-th                observation
    * $\bar{x}$ = The  mean of all values of the feature $X$
    * $\bar{y}$ = The mean of all values of the Target $Y$
    * $n$ = The total number of paired observations
    * $\sum_{i=1}^{n}$ = The summation over all observations from $i$ = 1        to $n$
    * $\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^{2}}$ = the standard deviation       for the feature $X$
    * $\sqrt{\sum_{i=1}^{n} (y_i - \bar{y})^{2}}$ = the standard deviation       for the target $Y$

    To assess which features would be most important to the model, and to reduce the number of features Realinho et al. (2022) used the Permutation Feature Importance technique. Permutation Feature importance is calculated by oberving differences in model error (i.e., decreases or increases) when feature values are permutated. If premutating feature values causes big differences  in the model error, the feature is important to the overal model (see formula below).
$$
PI_j = E_{\text{perm}(j)} - E_{\text{base}}
$$
    
    
    Where:
      * $PI_j$ = the permutation importnace score for the feaure $_j$
      * $E_{\text{base}}$ = the base model error (i.e., the performance            metric) computed on the orginal dataset
      * $E_{\text{perm}(j)}$ = the model error after random permutation of         the values         of feature $_j$
      * $j$ = the index of the feature being evaluted
      * $E$ = the prediction error metric used for evaluation (e.g., mean          squared error, log loss or error rate)

    Concerning the target, Realinho et al. (2022) noted that there was significant class imbalnce, for example, the class label "Graduate" at (n = 2,209) represented 50% of the sample, while "Dropout" (n = 1,421) represented 32%, and "Enrolled" (n = 794) represented 18%. Thus, to assess the error when conducting permutation Realinho et al. (2022), used F1-score as their metric, which they asserted is most adequate when dealing with an imbalanced data set. The F1-score measures the accurcy of classification models assessing how well they predict the positive class by balancing both precision and recall. In the context of selecting "important" features, if permutation causes significant decreases in F1-score, then a feature is deemed important (see formula below).
$$
F_1 = \frac{2PR}{P + R}
$$
    
     Where:
      * $F_1$ = The $F1$-score which is the harmonic mean of precision             and recall
      * $P$ = Precision, which is the proportion of predicted postives            that are true positives
      * $R$ = Recall, which is  the ratio of acutal postives that are              correctly predicted
      * $2$ = The weighting factor that yeilds the harmoic mean and it             penalizes extreme imbalances between $P$ and $R$

      According to Realinho et al. (2022) Permutation Feature Importance was applied to all models suggested by the literature for multi-class prediction, for example Random Forest(RF), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and Catagotical Boosting (CatBoost). RF is considred an ensemble learning method that builds upon many decision trees and then aggregates their prediction thus improving model accuracy while reducing overfitting (Ho,1995). XGBoost operates on the premise of gradient boosting and builds trees in a sequential manner where each new tree corrects the errors of the previous tree (Chen & Guestrin, 2016). LightGBM also operates using a gradient boosting method but uses a histogram-based tree framework while it learns to scale large datasets (Ke et al., 2017). Finally, CatBoost also uses a gradient boosting method but is specfically designed to handle catagorical features (Prokhorenkova et al., 2018). See below for the formulas of all multi-class classifiers used by Realinho et al. (2022) in their study.
      
-   **Random Forest Formula**
$$
\hat{y} = \frac{1}{B}\sum_{b=1}^{B} T_b(x)
$$

    Where:
      * $\hat{y}$ = The final predicted out of the RF model
      * $B$ = The total number of decision trees in the RF
      * $T_b(x)$ = The prediction of $b$-th decision tree for input $x$
      * $x$ = The feauture vector (i.e., the feature observation)
      * $\sum_{b=1}^{B}$ = The aggregation accross all trees
    
    
-   **Extreme Gradient Boosting Formula**
$$
\hat{y}_i = \sum_{k=1}^{K} f_k(x_i), 
\quad f_k \in \mathcal{F}
$$

    Where:
      * $\hat{y}_i$ = The pridcited output for the observation $i$
      * $K$ = The number of trees (i.e., the number of boosting                    interations)
      * $f_k(x_i)$ = The prediction from the $k-th$ decision tree
      * $x_i$ =  The feature vector for the obeservation $i$
      * $F$ = The space of all possible decision trees
      
-   **Light Gradient Boosting Machine Formula**
$$
\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + \eta f_t(x_i)
$$

    Where:
      * $\hat{y}_i^{(t)}$ = The updated observation $i$ at interation $t$
      * $\hat{y}_i^{(t-1)}$ = The prediction from the previous boosting intertation
      * $\eta$ = The learning rate while controlng for the contribution of         each new tree
      * $f_t(x_i)$ = The prediction of the tree added at interation $t$
      * $x_i$ = The feature vector for observation $i$
      
-   **Catagorical Boosting Formula**
$$
\hat{y}_i = \sum_{t=1}^{T} \eta f_t(x_i)
$$

    Where:
      * $\hat{y}_i$ = The predicted output for observation $i$
      * $T$ = The total number of trees (i.e., boosting interations)
      * $\eta$ = The learning rate
      * $f_t(x_i)$ = The predcition from the tree at interation $t$
      * $x_i$ = The input vector including the catgorical variables

    The methods used by Realinho et al. (2022) more than adequatly address their research question. Starting with the data ingestion process, by compiling a data set from both sources native to the universty and external, they assured that features in the data set would afford them the oppurtunity to gain both a macro and micro perspective conerning the phenomena of student dropout and acadmic success. Further, in dealing with class imbalance, by utalizing Permutation Feature Importance, Realinho et al. (2022) assured that only features that contributed to the postive predciton of the class would be inlcuded in their model thus imporvng model fit and accuracy. Finally by incorparting various multi-class calssifieres into their pipeline, Realinho et al. (2022) assured that they would be able to conduct model comparison and select the best model for their data set.  

-   **Data Balancing Methods**
    
     Mduma (2023) used two publicly available datasets from developing countries to test their methods. The first data set stemmed from Uwezo an non-governmental organization (NGO) in Tazania, and the second datasetwas dervied from india. The Uwezo dataset consisted information on student learning at the country level gathered from thousands of households Mduma (2023). The Uwezo data set consited on (n = 61,340) cases, and concering the classes 98.4% were in the retained class and 1.6% were in the dropout class. The dataset from India concsited of (n = 11, 257) cases of which 95.1% were in the retained class, and 4.9% were in the dropout class (Mduma, 2023). Both datasets consited of festures that spannded demograhics, acadmics and socioeconomic catagories. Mduma (2023), asserted that they condcuted pre-processing of both sets, specifically, they cleaned the sets for missing values by implementing median and zero imputting where necessary.
    
    According to Mduma (2023), five data balancing techniques were used on the datasets to address the class imbalance; 1. Random Under Sampleing (RUS), 2. Random Over Sampling (ROS), 3. Synthetic Minority Over Sampling (SMOTE), 4. Synthetic Minority Over Sampling with Edited Nearest Neighbors (SMOTE ENN), and 5. Sythentc Minority Over Sampling with TOMEK Links (SMOTE TOMEK). Concerning RUS, this techniques works by randomly removing observations from the majority class until the classes are approximatley equal. ROS, is the oposite of RUS, and works by randomly duplicating the minority class until the classes are approximatly equaly. SMOTE works by generating a new synthetic minority class, which is done by interporlating the minority class using the nearest neighbor until both the minority and majority class match. SMOTE ENN works the same as SMOTE but adds a cleaning step that removes misclassfied classes due to the nearest neighbor step, this reduces statistical noise and overlap. Finally, SMOTE TOMEK also works the same as SMOTE but uses TOMEK  links, which remove pairs of  nearest neighbors instances from the classes the majority class that are in close proximity to eachother. The decision boundry is cleaned thus reducing statistical noise and improving model performance Mduma (2023). The formulas for all class blancing tehcniues incorparted by Mduma (2023) are below.
    
    
-   **Random Under Sampling Formula**
$$
\alpha_{us} = \frac{N_{m}}{N_{rM}}
$$

    Where:
      * $\alpha_{us}$ = The under sampling ratio
      * $N_m$ = The number of class samples in the minority 
      * $N_{rM}$ = The number of majority class samples after under sampling
      
    
-   **Random Over Sampling Formula**
$$
\alpha_{os} = \frac{N_{r m}}{N_{m}}
$$

    Where:
      * $\alpha_{os}$ = Is the oversampling ration
      * $N_m$ = The orginal number of the minority class samples
      * $N_{rM}$ = The number of samples in the minority class after over-sampling
      
      
-   **Synthetic Minority Oversampling Formula**
$$
x_{\text{new}} = x_i + \lambda \left(x_{nn} - x_i\right)
$$


    Where:
      * $x_{new}$ = The newly generated synthtic minority class sample
      * $x_i$ = The original minority class instance
      * $x_{nn}$ = Is on of the $k$-nearest neighbors of $x_i$
      * $\lambda$ = Is the random interporlation where 0 $\le$ $\lambda$ $\le$ 1
      * $k$ = The total number of nearest neighbors considered
      
      
-   **Synthetic Minority Oversampling with Edited Nearest Neighbors Formula**
$$
D^{*} = \text{ENN}\bigl(\text{SMOTE}(D)\bigr)
$$


    Where:
      * $D$ = The original imbalanced dataset
      * $SMOTE(D)$ = The dataset after synthetic minority over samples are generated
      * $ENN(.)$ = The rule that edits the nearest neighbors that may be missclassfied         or ambigious
      * $D^{*}$ = The final cleaned and balanced dataset
      
      
-   **Synthetic Minority Oversampling with TOMEK Links Formula**
$$
D^{*} = D_{\text{SMOTE}} \setminus TL
$$


    Where:
      * $D_SMOTE$ = The dataset after minority synthetic over sampling
      * $TL$ = The TOMEK link sets between the minority and majority classes
      * $\backslash$ = The removal operator (i.e.,set subtraction)
      * $D^{*}$ =  The final balanced dataset after overlapping TOMEK links are               removed
      
      
    In addtion to the five data blancing techniques mentioned above, Mduma (2023), asserted that they also used three seperate classification models (i,e, LR, RF, and MLP) to assess which when paired with a datablancing technique would yeild the most optimal results. Using an experimental methodlogy, the three models were compared and assesed for accuracy in six intances; 1. On the orginal unblanced dataset, 2. on the ROS balnaced dataset, 3. RUS balanced dataset, 4. SMOTE balanced dataset, 5. SMOTE ENN balanced dataset, and 6. SMOTE TOMEK balanced dataset. Concerning model evalution specifically for measures of accuracy, Mduma (2023), asserted that chose 1. Geometric Mean ($G_m$), F-measure ($F_m$), and Adjacent Geometric Mean ($AG_m$). According to Mduma (2023), these measures of model accuracy were choosen as they are particilialry robust against unblanced datasets. The ($G_m$) works by measuring the balance between  sensitivity and specificity and rewards models that perfrom well on both minority and majority classes. The ($F_m$) measures the harmoic mean of precision and recall and rewards models that show a balanced perfromance on the positive class. The ($AG_m$) is an extention of the ($G_m$) and works by adotping class imbalance calculations penalizing poor minority class performance Mduma (2023). Finally, accroding to Mduma (2023), both datasets were split into training, validation and testing sets at 60%, 20% and 20% respectively. For all model formulas and mesures of accuracy formulas used by Mduma (2023) in their study see below.
    
    
-   **Logestic Regression Formula**
$$
P(y=1 \mid x) = \sigma(w^T x + b)
$$

    Where The Sigmoid (i.e., the activation function) is:
$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

    Where:
    
      * $P(y = 1 | x)$ = The probality of a class 1 given the input vector $x$
      * $\sigma(.)$ = The sigmiod or the logestic function
      * $w$ = The weight vector
      * ${w^T x}$ = The dot product between the weights and the features
      * $x$ = The input fature vector
      * $b$ = The bias or intercept term
      * $z$ = Is the ${w^T x}$ + $b$  which = the linear predictor
      * $e$ = Euler's number which is the base of the logarithim
      
    
-   **Multilayer Perceptron Formula**
$$
a^{(l)} = \sigma\left(W^{(l)} a^{(l-1)} + b^{(l)}\right)
$$

    Where:
      * $a^{(l)}$ = The activation vector or output of layer $l$
      * $W^{(l)}$ = The weight matrix for layer $l$ which maps inputs from preious            layers
      * $a^{(l-1)}$ = The acti activation layer from the previous vector ($l$ - 1)
      * $b^{(l)}$ = The bias vector of layer $l$
      * $\sigma(.) = The activation function applied element wise (e.g., sigmoid,             ReLU or tanh)
      * $l$ = The index representing the current layer in the network
      
      
-   **Geometric Mean Formula**
$$
G_m = \sqrt{\text{TPR} \times \text{TNR}}
$$

    Where:
      * $G_m$ = The geometric mean score
      * $TPR$ = The true positive rate of (i.e.,sensitivity or recall)
      * $TNR$ = The true negative rate (i.e, specificity)
    
    And:
$$
\text{TPR} = \frac{TP}{TP + FN}
$$
$$
\text{TNR} = \frac{TN}{TN + FP}
$$

    Where:
      * $TP$ = True Positive
      * $TN$ = True Negative
      * $FP$ = False Positive
      * $FN$ = False Negative
      
      
-   **F-Measure Formula**
$$
F_m = \frac{2PR}{P + R}
$$

    Where:
      * $F_m$ = The F-measure or the F1-Score
      * $P$ = Precision
      * $R$ = Recall


    And:
$$
P = \frac{TP}{TP + FP}
$$
$$
R = \frac{TP}{TP + FN}
$$

    Where:
      * $TP$ = True Positive
      * $FN$ = False Negative
      * $FP$ = False Positive
      
      
-   **Adjusted Geometric Mean Formula**
$$
AG_m = \sqrt{\text{TPR} \times \text{TNR} \times (1 - IR)}
$$

    Where:
      * $AG_m$ = The adjusted geometric mean
      * $TPR$ = The true positive rate (i.e., recall)
      * $TNR$ = The true negative rate (i.e., specificity)
      * $IR$ = The imbalance ratio
    
    And:
$$
IR = \frac{N_{\text{min}}}{N_{\text{maj}}}
$$

    Where:
      * $IR$ = The imbalanced ratio
      * $N_{min}$ = The number of samples in the minority class
      * $N_{maj}$ = The number of samples in the majority class



## Dataset and Access

-   ***Dataset Characteristics***

    As mentioned previously the dataset I am using stems from the UC Irvine Machine Learning Repository and it is called "Predecting Students' Dropout and Academic Success"(Realinho et al., 2021). Realinho et al. (2021) characterise the dataset as tubular, meant for the social sciences, and spefically classification based projects. There are (n = 4,424) cases and (n = 36) features in the dataset. The feature types according to Realinho et al. (2021) concist of real, catagorical and integers. According to Realinho et al. (2021) the dataset was created specifically to help researchers interested in studying the factors that contribute to student dropout or academic failure in higher education using machine learning methods. There target has three catagories (i.e., dropout, enrolled and graduate). According to Realinho et al. (2021) the dataset was rigorously  preprocessed and consist of no null or missing values. 
    
-   ***Dataset Access***
    
    The dataset can be accessed here : ([UCI Student Dropout Dataset](https://www.kaggle.com/datasets/willatran/original-dataset-for-pss4e6-from-uci-ml-repo/data))


##  Implementation and Experiments

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from matplotlib.ticker import MaxNLocator
from sklearn.feature_selection import mutual_info_classif
from sklearn.model_selection import StratifiedKFold, cross_val_score
import seaborn as sns
from math import ceil

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
```
```{python}

```
```{python}

```


### Implementation
  
### Experiments
  
##  Results (preliminary)

##  Issues and Limitations

##  Next Steps



#### 

##                               Reference

Aina, C., Baici, E., Casalone, G., & Pastore, F. (2022). The determinants of university dropout: A review of the *socio-economic literature. Socio-Economic Planning Sciences, 79, Article 101102.* https://doi.org/10.1016/j.seps.2021.101102

Andrade-Giron, D., et al.(2023). Predicting student dropout based on machine learning  and deep learning: A systematic review. *ICST Transaction on Scalable Information Systems, 10(5),1-11.*
https://doi.org/10.4108/eetsis.3586

Bargmann, C., Thiele, L., & Ksuggrld, S. (2022). Motivation matters: Predicting students’ career decidedness and intention to drop out after the first year in higher education. *Higher Education, 83(4), 845-861.* https://doi.org/10.1007/s10734-021-00707-6

Barramuno, M., Meza-Narvaez, C., & Galvez-Garcia, G. (2022). Prediction of student attrition risk using machine learning. *Journal of Applied Research in Higher Education, 14(3), 974-986.* https://doi.org/10.1108/JARHE-02-2021-0073

Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 785–794). Association for Computing Machinery. https://doi.org/10.1145/2939672.2939785

Delen, D., Davazdahemami, B., & Rasouli Dezfouli, E. (2024). Predicting and mitigating freshman student attrition: A local-explainable machine learning framework. Information Systems Frontiers, 26(2), 641-662. https://doi.org/10.1007/s10796-023-10397-3

Duan, D., Dai, C., & Tu, r. (2021). Research on the predicion of students' academic performance based on XGBoost. In *Proceedings of the 2021 Tenth International Conference on Educational Innovation through Technology (EITT)* (pp.316-319). IEEE https://doi.org/10.1109/EITT53287.2021.00068

Ho, T. K. (1995). Random decision forests. In Proceedings of the Third International Conference on Document Analysis and Recognition (Vol. 1, pp. 278–282). IEEE. https://doi.org/10.1109/ICDAR.1995.598994

Huo, H. et al. (2023). Predicting dropout for nontraditional undergraduate students: A machine learning approach. *Journal of College Student Retention:Research, Theory & Practice*, 24(4), 1054-1077.https://doi.org/10.1177/1521025120963821

Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., & Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. In I. Guyon et al. (Eds.), *Advances in Neural Information Processing Systems* (Vol. 30, pp. 3147–3155). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html

Kok, C. L., Ho, C.K., Chen, L., Koh, Y. Y., & Tian, B. (2024). A novel predictive modeling for student attrition utilizing machine learning and sustainable big data analytics. *Applied Sciences, 14(21), Article 9633.* https://doi.org/10.3390/app14219633

Mduma, N. (2023). Data balancing techniques for predicting student dropout using machine learning. Data, 8(3), Article 49. https://doi.org/10.3390/data8030049

Mun, J., & Jo, M. (2023). Applying machine learning-based models to prevent university student dropouts. *Journal of Educational Evaluation*, 36(2), 289–313. https://doi.org/10.31158/JEEV.2023.36.2.289

Namoun, A., & Alshanqiti, A. (2020). Predicting student performance using data mining and learning analytics techniques: A systematic literature review. Applied Science, 11(1), Article 237. https://doi.org/10.3390/app11010237

Pek, R.Z., Ozyer, S.T., Elhage, T., Ozyer, T., & Alhajj, R. (2023). The role of machine learning in identifying student at-risk and minimizing failure. *IEEE Access, 11, 1224-1243.* https://doi.org/10.1109/ACCESS.2022.3232984

Parjwal, P., L. R., S., & V., K. (2024). Forcasting student attrition using machine learning. In *Proceedings of the 2024 4th Asian Conference on Innovation in Technology (ASIANCON)* (pp.1-7) IEEE.
https://doi.org/10.1109/ASIANCON62057.2024.10838214

Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: Unbiased boosting with categorical features. In *Advances in Neural Information Processing Systems* (Vol. 31, pp. 6638–6648). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2018/hash/14491b756b3a51daac41c24863285549-Abstract.htm

Raschka, S., Liu, Y. (H.), & Mirjalili, V. (2022). Machine learning with PyTorch and scikit-learn: Develop machine learning and deep learning models with Python. Packt Publishing.

Realinho, V., Machado, J., Baptista, L., & Martins, M. V. (2022). Predicting student dropout and academic success. Data, 7(11), Article 146. https://doi.org/10.3390/data7110146

Ridwan, A., Priyatno, A. M., & Ningsih, L. (2024). Predict Students' Dropout and Academic Success with XGBoost. *Journal of Education and Computer Applications*,*1*(2), 1-8.https://doi.org/10.69693/jeca.v1i2.13

Veliz Palomino, J.C., & Ortega, A. M. (2023). Dropout intentions in higher education: Systematic literature review. *Journal of Efficacy and Responsibility in Education and Science, 16(2), 149-158.*
https://doi.org/10.7160/eriesj.2023.160206

Yan, K. (2021). Student performance prediction using XGBoost method from macro perspective. In *Proceedings of the 2021 2nd International Conference on Computing and DataScience(CDS)*(pp.453-459).IEEE.https://doi.org/10.1109/CDS52072.2021.00084






