---
title: "Multi-Class Machine Learning Approaches to Student Dropout"
subtitle: "A Comparative Study of Classifiers and Data Balancing Techniques"
author: "Kareem D. Piper (Advisor: Dr.Shusen Pu)"
date: '`r Sys.Date()`'

format:
  html:
    toc: true
    toc-title: "Table of Contents"
    toc-location: right
    toc-depth: 2
    number-sections: true
    code-fold: true

course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
csl: apa.csl
self-contained: true

execute:
  echo: true
  warning: false
  message: false

editor:
  markdown:
    wrap: 72
---
Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Placeholder**
:::

## Introduction

The goal of this capstone project is to test, assess and report the results various multi-class machine learning prediction models on their ability to accuratly predict student dropout in the higher edcation setting. According to @realinho2022 student attrition and academic failure  not only negatively affect the education institutions that students attend, but create greater  societal issues. For example, when students dropout prior to completing a degree program it makes them less competitive concerning the job market, which then leads to economic deficiencies, which can lead to inadequate health care access and subsequent more dire socio-economic issues that often disproportionally affect marginalized demographics [@realinho2022]. 

According to @aina2022 statistics released by the Organization of Economic Co-Operaton and Development (OECD) state that though the proportion of students enrolling and graduating colleges and universities far exceeds those dropping out, dropout still represents a third of said students. Further, @aina2022  assert that thirty percent of studnets in united States dropout, with many of them being early dropouts (i.e., first year of college). Student dropout is a complex issues and there are many factors that attribute to it [@aina2022]. The factors that attibute to student dropout boarder sociological, economical and psychological domains as such according to @aina2022 scholars approach the phenomina from their respective fields of expertese. There is a need for a more holistic lens when researching the causes of student dropout to better understand the relationships between the factors that attibute to it thus gaining insights on how best to mitigate it through intervention [@aina2022].

Machine Learning provides the ability for reseachers to approach student dropout in a holsitic manner specifically becuase models can converge high dimensional data sets [@raschka2022]. Researchers  [e.g.,@realinho2022; @ridwan2024] have utalized machine learning methods to predict student dropout.  However, according to @mduma2023, the prediction of student dropout is a particularly challanging issues for education researchers, this is due in large part to class imblance. According to @mduma2023
 many real world datasets concerning studnet dropout are largly skewed towards the reatined or enrolled classes. Many scholars pay particuliar attention to feature enigineering methods when using machine learning methodologies to address the issue of predicting student attrition. However, @mduma2023 asserted that class imbalance presents limtations concerning model accuracy and generalizabiltiy and thus framed their study aorund that issue. Thus, in this capstone project, I focus on the appropriate data preprocessing methods, feature engineering methods, and methods to adequatly address class imbalnce. Further, I specifically use machine learning models that have a proven track record in the literature [e.g.,@realinho2022; @ridwan2024] concerning their abiltiy to address multi-class datasets, for example Random Forest(RF), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM) and Catagorical Boosting (CatBoost).

    
## Research Questions
  
Primary Research Question
    
RQ1: Which multi-class machine learning classifier achieves the highest predictive performance as meaured by; accuracy, precesion, and F1-score when applied to the ([UCI Student Dropout Dataset](https://www.kaggle.com/datasets/willatran/original-dataset-for-pss4e6-from-uci-ml-repo/data))?
    
To inform the primary research question of this study I asked the following sub-research questions:
    
RQ1a: How do methods of addressing class imbalance affect the predictive performance of multi-class machine learning classfiers?
    
RQ1b: How does the performance of gradient boosting models compare to tradtional ensable models in multi-class student attrition prediciton?
  


<!-- -->

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

Example of writing including citing references:

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{python}
import pandas as pd
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References


